{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoRDpFxvxHup"
      },
      "source": [
        "#Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzegKBD_xJlE",
        "outputId": "52424bb5-9a1a-4982-9774-8588e92d1ee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/330.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m215.0/330.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/248.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.1/248.1 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "langchain \\\n",
        "langchain_core \\\n",
        "torch \\\n",
        "InstructorEmbedding \\\n",
        "chromadb \\\n",
        "huggingface-hub \\\n",
        "pypdf==3.7.1 \\\n",
        "tokenizers==0.14.0 \\\n",
        "sentence-transformers==2.2.2 \\\n",
        "tiktoken \\\n",
        "streamlit \\\n",
        "streamlit_chat \\\n",
        "streamlit_extras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqLPjH5O1OCw",
        "outputId": "aea93e2a-4081-4516-bd22-4aae8fb35c04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.38.tar.gz (10.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/10.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/10.7 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/10.7 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m168.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.9.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.4)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.38-cp310-cp310-manylinux_2_35_x86_64.whl size=9660813 sha256=7d175d205d7d6c963cfdec39994fb54ad28e2a32a67f894f667d0c608db558dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/58/77/20d3d9a235b4930050fbcde1ad4f0a4d054644269e801b08aa\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.2.38\n"
          ]
        }
      ],
      "source": [
        "# get GPU to run LLLM\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrfaUoSM4XDl",
        "outputId": "10b730e5-76ac-4080-e538-8274b9f5023f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25h+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 2.786s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noHmb0azzDoX",
        "outputId": "fdc937b8-7e6a-43c8-9649-99fd69cb5681"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Path to your PCB folder in Google Drive\n",
        "DRIVE_FOLDER = '/content/drive/My Drive/PCB/EN_docs'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SFrPHUF8J8h"
      },
      "source": [
        "# auxiliary.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWJyx69P54ls",
        "outputId": "4a9fc8fc-1e69-4817-8f0b-44289070e31a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing auxiliary.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile auxiliary.py\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.vectorstores import VectorStoreRetriever\n",
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain_core.documents.base import Document\n",
        "from langchain_community.document_transformers import LongContextReorder\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.chains.conversational_retrieval.base import _get_chat_history\n",
        "from langchain.callbacks.manager import (\n",
        "    AsyncCallbackManagerForChainRun,\n",
        "    CallbackManagerForChainRun,\n",
        "    Callbacks,\n",
        ")\n",
        "\n",
        "import torch\n",
        "import inspect\n",
        "from sentence_transformers import CrossEncoder\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "########################\n",
        "# MultiQuery Retriever #\n",
        "########################\n",
        "\n",
        "# Output parser will split the LLM result into a list of queries\n",
        "class LineList(BaseModel):\n",
        "    # \"lines\" is the key (attribute name) of the parsed output\n",
        "    lines: List[str] = Field(description=\"Lines of text\")\n",
        "\n",
        "\n",
        "class LineListOutputParser(PydanticOutputParser):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__(pydantic_object=LineList)\n",
        "\n",
        "    def parse(self, text: str) -> LineList:\n",
        "        lines = text.strip().split(\"\\n\")\n",
        "        return LineList(lines=lines)\n",
        "\n",
        "\n",
        "output_parser = LineListOutputParser()\n",
        "\n",
        "MULTIQUERY_TEMPLATE = \"\"\"<s>[INST] You are an AI language model assistant.\n",
        "Your task is to generate 3 different search queries that aim to answer the user question from multiple perspectives.\n",
        "The user questions are focused on Banking, Finance, and related disciplines.\n",
        "Each query MUST tackle the question from a different viewpoint, we want to get a variety of RELEVANT search results.\n",
        "Provide these alternative questions separated by newlines. [/INST]\n",
        "\n",
        "Original question: {question}\n",
        "\"\"\"\n",
        "\n",
        "MULTIQUERY_PROMPT = PromptTemplate(\n",
        "    input_variables=['question'],\n",
        "    template=MULTIQUERY_TEMPLATE,\n",
        "    )\n",
        "\n",
        "\n",
        "####################################\n",
        "# Expansion with Answer Generation #\n",
        "####################################\n",
        "\n",
        "QUERY_EXPANSION_TEMPLATE = \"\"\"<s>[INST]\n",
        "Offer a concise and factual answer to the question. The response should be brief and without conversational tone, empathy, greetings, or personal comments.\n",
        "[/INST]\n",
        "\n",
        "QUESTION: {new_question}\n",
        "\n",
        "Possible Answer:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "QUERY_EXPANSION_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"new_question\"],\n",
        "    template=QUERY_EXPANSION_TEMPLATE,\n",
        ")\n",
        "\n",
        "# Retriever that processes the initial query through an LLM Chain\n",
        "class CustomRetriever(VectorStoreRetriever):\n",
        "    def __init__(self, vectorstore: VectorStoreRetriever, chain: LLMChain, **kwargs):\n",
        "        super().__init__(vectorstore=vectorstore, **kwargs)\n",
        "        self.vectorstore = vectorstore\n",
        "        self.chain = chain\n",
        "\n",
        "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
        "        output = self.chain.invoke(query)\n",
        "        new_query = output['new_question'] + \"\\n\" + output['query']\n",
        "        results = self.vectorstore.get_relevant_documents(query=new_query)\n",
        "        return results\n",
        "\n",
        "#######################\n",
        "# Follow-up Question #\n",
        "#######################\n",
        "\n",
        "QUESTION_GENERATOR_TEMPLATE = \"\"\"<s>[INST]\n",
        "Use the chat history to clarify and simplify the follow-up question, ensuring it remains true to the original intent without adding extra details.\n",
        "[/INST]\n",
        "\n",
        "Chat History: {chat_history}\n",
        "\n",
        "Follow-up Question: {question}\n",
        "\n",
        "Refined Standalone Question:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "QUESTION_GENERATOR_PROMPT = PromptTemplate(\n",
        "    template=QUESTION_GENERATOR_TEMPLATE,\n",
        "    input_variables=[\"chat_history\", \"question\"],\n",
        "    )\n",
        "\n",
        "#############################\n",
        "# ChatBot Answer Generation #\n",
        "#############################\n",
        "\n",
        "ANSWER_TEMPLATE_BDP = \"\"\"<s>[INST]\n",
        "Role:\n",
        "-Act as an AI chat assistant that has been trained exclusively to answer questions about the Bank Customer Website.\n",
        "-Your goal is to respond to the users' questions using ONLY the information present on the provided [CONTEXT] below.\n",
        "\n",
        "Rules:\n",
        "-Make a step by step reasoning about the question.\n",
        "-The response should be concise and clear.\n",
        "-Don't mention that you have access to [CONTEXT] or a list of facts.\n",
        "-You should always refuse to answer questions that are not related to [CONTEXT].\n",
        "-You SHOULD NOT give any advice or recommendation to the user.\n",
        "-If the question is out of the [CONTEXT] domain you should briefly and politely respond to the user that you don't have the information to provide the answer.\n",
        "[/INST]\n",
        "\n",
        "[CONTEXT]:\n",
        "{context}\n",
        "\n",
        "[QUESTION]:\n",
        "{question}\n",
        "\n",
        "Helpful Answer:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "ANSWER_PROMPT_BDP = PromptTemplate(\n",
        "        template=ANSWER_TEMPLATE_BDP, input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "ANSWER_TEMPLATE_DEFAULT = \"\"\"<s>[INST] Use the following pieces of context to answer the question at the end.\n",
        "If the context doesn't provide enough information to answer the question, just say that you don't know, don't try to make up an answer.\n",
        "Keep the answer as concise as possible. [/INST]\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Helpful Answer:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "ANSWER_PROMPT_DEFAULT = PromptTemplate(\n",
        "        template=ANSWER_TEMPLATE_DEFAULT, input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "###################################################\n",
        "# Custom Chain with follow-up question generation #\n",
        "###################################################\n",
        "\n",
        "def reranking(reranker_type, retrieved_docs, question):\n",
        "\n",
        "    if reranker_type == \"LongContextReorder\":\n",
        "        reranker = LongContextReorder()\n",
        "        reranked_docs = reranker.transform_documents(retrieved_docs)\n",
        "\n",
        "    elif reranker_type == \"ms-marco-MiniLM-L-6-v2\":\n",
        "        cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device=device)\n",
        "        # Get scores for each document\n",
        "        scores = cross_encoder.predict([[question, doc.page_content] for doc in retrieved_docs])\n",
        "        # Sort the documents by their scores\n",
        "        reranked_docs = [doc for doc, score in sorted(zip(retrieved_docs, scores), key=lambda x: x[1], reverse=True)]\n",
        "\n",
        "    elif reranker_type == \"bge-reranker-base\":\n",
        "        cross_encoder = CrossEncoder('BAAI/bge-reranker-base', device=device)\n",
        "        # Get scores for each document\n",
        "        scores = cross_encoder.predict([[question, doc.page_content] for doc in retrieved_docs])\n",
        "        # Sort the documents by their scores\n",
        "        reranked_docs = [doc for doc, score in sorted(zip(retrieved_docs, scores), key=lambda x: x[1], reverse=True)]\n",
        "\n",
        "    elif reranker_type == \"None\":\n",
        "        reranked_docs = retrieved_docs\n",
        "\n",
        "    return reranked_docs\n",
        "\n",
        "class CustomConversationalRetrievalChain(ConversationalRetrievalChain):\n",
        "    reranker_type: Optional[str] = Field(default=None, description=\"The type of reranker to use\")\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        inputs: Dict[str, Any],\n",
        "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
        "        ) -> Dict[str, Any]:\n",
        "          _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n",
        "          question = inputs[\"question\"]\n",
        "          get_chat_history = self.get_chat_history or _get_chat_history\n",
        "          chat_history_str = get_chat_history(inputs[\"chat_history\"])\n",
        "\n",
        "          callbacks = _run_manager.get_child()\n",
        "          new_question = self.question_generator.run(\n",
        "              question=question, chat_history=chat_history_str, callbacks=callbacks\n",
        "          )\n",
        "          accepts_run_manager = (\n",
        "              \"run_manager\" in inspect.signature(self._get_docs).parameters\n",
        "          )\n",
        "          if accepts_run_manager:\n",
        "              docs = self._get_docs(new_question, inputs, self.reranker_type, run_manager=_run_manager)\n",
        "          else:\n",
        "              docs = self._get_docs(new_question, inputs, self.reranker_type)  # type: ignore[call-arg]\n",
        "          output: Dict[str, Any] = {}\n",
        "          if self.response_if_no_docs_found is not None and len(docs) == 0:\n",
        "              output[self.output_key] = self.response_if_no_docs_found\n",
        "          else:\n",
        "              new_inputs = inputs.copy()\n",
        "              if self.rephrase_question:\n",
        "                  new_inputs[\"question\"] = new_question\n",
        "              new_inputs[\"chat_history\"] = chat_history_str\n",
        "              answer = self.combine_docs_chain.run(\n",
        "                  input_documents=docs, callbacks=_run_manager.get_child(), **new_inputs\n",
        "              )\n",
        "              output[self.output_key] = answer\n",
        "\n",
        "          if self.return_source_documents:\n",
        "              output[\"source_documents\"] = docs\n",
        "          if self.return_generated_question:\n",
        "              output[\"generated_question\"] = new_question\n",
        "          return output\n",
        "\n",
        "    def _get_docs(\n",
        "        self,\n",
        "        question: str,\n",
        "        inputs: Dict[str, Any],\n",
        "        reranker_type: str,\n",
        "        run_manager: CallbackManagerForChainRun,\n",
        "    ) -> List[Document]:\n",
        "        \"\"\"Get docs.\"\"\"\n",
        "        retrieved_docs = self.retriever.get_relevant_documents(\n",
        "            question, callbacks=run_manager.get_child()\n",
        "        )\n",
        "        docs = reranking(self.reranker_type, retrieved_docs, question)\n",
        "        return self._reduce_tokens_below_limit(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykTyMXrq4NDB"
      },
      "source": [
        "# chatbot.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uvr18ztIek59",
        "outputId": "23ff0350-954e-48f5-9b01-9dc44e869947"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting chatbot.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile chatbot.py\n",
        "\n",
        "#################################################\n",
        "# Streamlit App\n",
        "#################################################\n",
        "\n",
        "import io\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import urllib\n",
        "import itertools\n",
        "import torch\n",
        "import tiktoken\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import pypdf\n",
        "import altair as alt\n",
        "\n",
        "import streamlit as st\n",
        "from streamlit_chat import message\n",
        "from streamlit_extras.colored_header import colored_header\n",
        "from streamlit_extras.add_vertical_space import add_vertical_space\n",
        "\n",
        "from sentence_transformers import CrossEncoder\n",
        "from urllib.error import URLError\n",
        "\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.schema import Document\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import QAGenerationChain, RetrievalQA\n",
        "from langchain.evaluation.qa import QAEvalChain, QAGenerateChain\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings, LlamaCppEmbeddings, OpenAIEmbeddings\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever, MultiQueryRetriever\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain_community.document_transformers import LongContextReorder\n",
        "\n",
        "from auxiliary import device, CustomRetriever, QUERY_EXPANSION_PROMPT, LineListOutputParser, LineList, MULTIQUERY_PROMPT, QUESTION_GENERATOR_PROMPT, ANSWER_PROMPT_BDP, ANSWER_PROMPT_DEFAULT, CustomConversationalRetrievalChain, reranking\n",
        "\n",
        "# Set Streamlit page configuration\n",
        "st.set_page_config(page_title='🧠BCW - ChatBot🤖', layout='wide')\n",
        "st.header(\"ChatBot🤖: BdP - Bank Customer Website\")\n",
        "st.subheader(\"Powered by 🦜 LangChain + 👑 Streamlit\")\n",
        "\n",
        "def get_query_params():\n",
        "    query_params = st.query_params\n",
        "\n",
        "    params = {\n",
        "        'oai_api_key': query_params.get('oai_api_key', 'None'),\n",
        "        'chunk_size': int(query_params.get('chunk_size', '256')),\n",
        "        'chunk_overlap': int(query_params.get('chunk_overlap', '20')),\n",
        "        'retriever_type': query_params.get('retriever_type', 'Similarity'),\n",
        "        'num_neighbors': int(query_params.get('num_neighbors', '3')),\n",
        "        'hybrid_weight': float(query_params.get('hybrid_weight', '0.0')),\n",
        "        'reranker_type': query_params.get('reranker_type', 'LongContextReorder'),\n",
        "        'embeddings': query_params.get('embeddings', 'bge-base-en-v1.5'),\n",
        "        'model': query_params.get('model', 'Mistral-7B-Instruct-v0.1 Small')\n",
        "    }\n",
        "    return params\n",
        "\n",
        "# Retrieve and display parameters\n",
        "params = get_query_params()\n",
        "st.markdown(\"### 🛠️ Configured Parameters\")\n",
        "# Create a grid layout for better visualization\n",
        "cols = st.columns(2)\n",
        "for i, (key, value) in enumerate(params.items()):\n",
        "    with cols[i % 2]:\n",
        "        st.markdown(f\"**{key.capitalize()}**: `{value}`\")\n",
        "\n",
        "\n",
        "# Assign parameters to variables\n",
        "oai_api_key = params['oai_api_key']\n",
        "chunk_size = params['chunk_size']\n",
        "chunk_overlap = params['chunk_overlap']\n",
        "retriever_type = params['retriever_type']\n",
        "num_neighbors = params['num_neighbors']\n",
        "hybrid_weight = params['hybrid_weight']\n",
        "embeddings = params['embeddings']\n",
        "reranker_type = params['reranker_type']\n",
        "model = params['model']\n",
        "\n",
        "\n",
        "def read_json_file(file_path):\n",
        "    \"\"\"\n",
        "    Reads a JSON file and returns its contents.\n",
        "    @param file_path: The path of the file to read.\n",
        "    @return: The contents of the JSON file or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            return json.load(file)\n",
        "    except IOError as e:\n",
        "        st.error(f\"Error reading file {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def enrich_document(entry, doc_id):\n",
        "    \"\"\"\n",
        "    Creates and enriches a Document object from a JSON entry.\n",
        "    @param entry: The JSON entry containing document data.\n",
        "    @param doc_id: The ID to assign to the document.\n",
        "    @return: The enriched Document object or None if entry is invalid.\n",
        "    \"\"\"\n",
        "    answer = entry.get('answer', '')\n",
        "    if answer == '':\n",
        "        return None\n",
        "    source = entry.get('link', '')\n",
        "    topic = entry.get('question') or entry.get('question_topic', '')\n",
        "    if source == \"https://clientebancario.bportugal.pt/en/perguntas-frequentes\":\n",
        "        topic_for_content = topic.split(\" -> \")[-1]\n",
        "        page_content = f\"{topic_for_content}\\n\\n{answer}\"\n",
        "    else:\n",
        "        page_content = f\"{topic}\\n\\n{answer}\"\n",
        "    return Document(page_content=page_content, metadata={\"source\": source, \"topic\": topic, \"doc_id\": doc_id})\n",
        "\n",
        "def update_level_2_documents(documents):\n",
        "    \"\"\"\n",
        "    Updates level 2 documents with corresponding level 3 topics.\n",
        "    @param documents: A list of Document objects.\n",
        "    @return: None. The function modifies the documents list in place.\n",
        "    \"\"\"\n",
        "    level_2_docs = {}\n",
        "    level_3_topics = []\n",
        "\n",
        "    # Categorize documents based on their level\n",
        "    for doc in documents:\n",
        "        topic = doc.metadata.get('topic', '')\n",
        "        level = topic.count('->') + 1\n",
        "        if level == 2:\n",
        "            level_2_docs[topic] = doc\n",
        "        elif level == 3:\n",
        "            level_3_topics.append(topic)\n",
        "\n",
        "    # Update level 2 documents if they have corresponding level 3 topics\n",
        "    for level_2_topic, level_2_doc in level_2_docs.items():\n",
        "        if level_2_doc.page_content.endswith(':'):\n",
        "            # Extract the base of the level 2 topic for matching with level 3 topics\n",
        "            base_topic = level_2_topic + ' -> '\n",
        "            matching_topics = [t.split(' -> ')[-1] for t in level_3_topics if t.startswith(base_topic)]\n",
        "            if matching_topics:\n",
        "                level_2_doc.page_content += '\\n' + '\\n'.join(matching_topics)\n",
        "\n",
        "def get_documents(folder_path=\"/content/drive/My Drive/PCB/EN_docs\"):\n",
        "    \"\"\"\n",
        "    Loads and processes documents from a specified folder.\n",
        "    @param folder_path: The path of the folder containing the documents.\n",
        "    @return: A list of processed Document objects.\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "    doc_id = 0\n",
        "\n",
        "    for file_name in sorted(os.listdir(folder_path)):\n",
        "        if file_name.endswith('.json'):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "            json_data = read_json_file(file_path)\n",
        "            if json_data:\n",
        "                for entry in json_data.get('content', []):\n",
        "                    doc = enrich_document(entry, doc_id)\n",
        "                    if doc:\n",
        "                        documents.append(doc)\n",
        "                        doc_id += 1\n",
        "\n",
        "    # Update level 2 documents with corresponding level 3 topics\n",
        "    update_level_2_documents(documents)\n",
        "\n",
        "    return documents\n",
        "\n",
        "def token_len(text):\n",
        "    tokenizer = tiktoken.get_encoding('cl100k_base')\n",
        "    tokens = tokenizer.encode(\n",
        "        text,\n",
        "        disallowed_special=()\n",
        "    )\n",
        "    return len(tokens)\n",
        "\n",
        "def get_chunks(documents, chunk_size, overlap):\n",
        "    \"\"\"\n",
        "    Split documents into chunks\n",
        "    @param text: documents to split\n",
        "    @param chunk_size: charecters per split\n",
        "    @param overlap: charecter overlap between chunks\n",
        "    @return: list of chunks\n",
        "    \"\"\"\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        "                                                chunk_size=chunk_size,\n",
        "                                                chunk_overlap=overlap,\n",
        "                                                length_function=token_len)\n",
        "    chunks = []\n",
        "    for doc in documents:\n",
        "        page_content = doc.page_content\n",
        "        metadata = doc.metadata\n",
        "        chunked_doc = text_splitter.create_documents(\n",
        "            texts=[page_content],\n",
        "            metadatas=[metadata]\n",
        "        )\n",
        "        for chunked_text in chunked_doc:\n",
        "            chunked_metadata = chunked_text.metadata\n",
        "            # remove chunks that only have the topic or the FAQ\n",
        "            if chunked_text.metadata['source'] == \"https://clientebancario.bportugal.pt/en/perguntas-frequentes\":\n",
        "                topic = chunked_text.metadata['topic'].split(\" -> \")[-1]\n",
        "            else:\n",
        "                topic = chunked_text.metadata['topic']\n",
        "            if chunked_text.page_content.strip() == topic.strip():\n",
        "                continue\n",
        "            else:\n",
        "                new_chunk = Document(page_content=chunked_text.page_content, metadata=chunked_metadata)\n",
        "                chunks.append(new_chunk)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def load_mistral(mistral_model):\n",
        "    \"\"\"\n",
        "    Load mistral model from path\n",
        "    @mistral_model: mistral model to load\n",
        "    \"\"\"\n",
        "\n",
        "    if mistral_model == \"Mistral-7B-Instruct-v0.1 Small\":\n",
        "        model_path = 'mistral-7b-instruct-v0.1.Q2_K.gguf'\n",
        "\n",
        "    elif mistral_model == \"Mistral-7B-Instruct-v0.1 Med\":\n",
        "        model_path = 'mistral-7b-instruct-v0.1.Q4_K_M.gguf'\n",
        "\n",
        "    llm = LlamaCpp(\n",
        "          model_path=f\"/content/drive/My Drive/PCB/mistral/{model_path}\",\n",
        "          temperature=0.0,\n",
        "          f16_kv=True,\n",
        "          top_p=1,\n",
        "          n_ctx=4096,\n",
        "          n_batch=1024,\n",
        "          n_gpu_layers=100)\n",
        "\n",
        "    return llm\n",
        "\n",
        "def get_llm(model):\n",
        "    \"\"\"\n",
        "    Get LLM\n",
        "    @param model: LLM to use\n",
        "    @return: LLM\n",
        "    \"\"\"\n",
        "\n",
        "    if model == 'GPT-4-turbo':\n",
        "        llm = ChatOpenAI(model_name=\"gpt-4-0125-preview\", temperature=0)\n",
        "\n",
        "    elif model == \"Mistral-7B-Instruct-v0.1 Small\" or model == \"Mistral-7B-Instruct-v0.1 Med\":\n",
        "        llm = load_mistral(model)\n",
        "\n",
        "    return llm\n",
        "\n",
        "\n",
        "def get_retriever(chunks, retriever_type, embedding_type, num_neighbors, llm, hybrid_weight):\n",
        "    \"\"\"\n",
        "    Get document retriever\n",
        "    @param chunks: list of Documents (chunks)\n",
        "    @param retriever_type: retriever type\n",
        "    @param embedding_type: embedding type\n",
        "    @param num_neighbors: number of neighbors for retrieval\n",
        "    @return: retriever\n",
        "    \"\"\"\n",
        "\n",
        "    # Set embeddings\n",
        "    if embedding_type == \"OpenAI\":\n",
        "        embd = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "    elif embedding_type in [\"bge-base-en-v1.5\", \"bge-small-en-v1.5\", \"bge-large-en-v1.5\"]:\n",
        "        model_name = f\"BAAI/{embedding_type}\"\n",
        "        embd = HuggingFaceInstructEmbeddings(model_name=model_name, model_kwargs={\"device\": device})\n",
        "\n",
        "    # Select retriever\n",
        "    if retriever_type == \"Similarity\":\n",
        "        vectorstore = Chroma.from_documents(chunks, embd)\n",
        "        retriever = vectorstore.as_retriever(search_type='similarity_score_threshold', search_kwargs={\"k\": num_neighbors, \"score_threshold\": 0.4})\n",
        "\n",
        "    if retriever_type == \"BM25\":\n",
        "        retriever = BM25Retriever.from_documents(chunks)\n",
        "\n",
        "    elif retriever_type == \"Hybrid-search\":\n",
        "        vectorstore = Chroma.from_documents(chunks, embd)\n",
        "        chroma_retriever = vectorstore.as_retriever(search_type='similarity_score_threshold', search_kwargs={\"k\": num_neighbors, \"score_threshold\": 0.4})\n",
        "        bm25_retriever = BM25Retriever.from_documents(chunks)\n",
        "        retriever = EnsembleRetriever(retrievers=[bm25_retriever, chroma_retriever],weights=[hybrid_weight,1-hybrid_weight])\n",
        "\n",
        "    elif retriever_type == \"Similarity + Expansion w/ Generated Answers\":\n",
        "        vectorstore = Chroma.from_documents(chunks, embd)\n",
        "        chain = LLMChain(llm=llm, prompt=QUERY_EXPANSION_PROMPT, output_key=\"query\")\n",
        "        retriever = CustomRetriever(vectorstore=vectorstore.as_retriever(search_type='similarity_score_threshold', search_kwargs={\"k\": num_neighbors, \"score_threshold\": 0.4}), chain=chain)\n",
        "        mq_retrieval_chain = LLMChain(llm=llm, prompt=MULTIQUERY_PROMPT, output_parser=LineListOutputParser())\n",
        "\n",
        "    elif retriever_type == \"MultiQuery Retriever\":\n",
        "        vectorstore = Chroma.from_documents(chunks, embd)\n",
        "        chain = LLMChain(llm=llm, prompt=MULTIQUERY_PROMPT, output_parser=LineListOutputParser())\n",
        "        retriever = MultiQueryRetriever(\n",
        "                    retriever=vectorstore.as_retriever(search_type='similarity_score_threshold', search_kwargs={\"k\": num_neighbors, \"score_threshold\": 0.4}),\n",
        "                    llm_chain=mq_retrieval_chain,\n",
        "                    parser_key=\"lines\",\n",
        "                    include_original=True\n",
        "                    )\n",
        "\n",
        "    return retriever\n",
        "\n",
        "\n",
        "# Run App\n",
        "\n",
        "if 'submitted' not in st.session_state:\n",
        "    st.session_state.submitted = False\n",
        "\n",
        "# Document Upload Section - Shown only if not submitted\n",
        "if not st.session_state.submitted:\n",
        "    st.subheader('Document Upload')\n",
        "    uploaded_docs = st.file_uploader(\"Upload documents (.pdf or .txt):\", type=['pdf', 'txt'], accept_multiple_files=True)\n",
        "    use_bp_example = st.checkbox(\"Use Banco de Portugal - Bank Customer Website\")\n",
        "    submit_selections = st.button('Submit Selections')\n",
        "    if submit_selections:\n",
        "        st.session_state.submitted = True\n",
        "        st.session_state.uploaded_docs = uploaded_docs\n",
        "        st.session_state.use_bp_example = use_bp_example\n",
        "\n",
        "# Processing after document submission\n",
        "if st.session_state.submitted:\n",
        "\n",
        "    with st.spinner(\"Loading ChatBot...\"):\n",
        "\n",
        "        if st.session_state.use_bp_example:\n",
        "            documents = get_documents()\n",
        "            chunks = get_chunks(documents, chunk_size, chunk_overlap)\n",
        "            prompt = ANSWER_PROMPT_BDP\n",
        "\n",
        "        if st.session_state.uploaded_docs:\n",
        "            combined_text, fnames = [], []\n",
        "            for file in sorted(st.session_state.uploaded_docs):\n",
        "                contents = file.read()\n",
        "                # PDF file\n",
        "                if file.type == 'application/pdf':\n",
        "                    pdf_reader = pypdf.PdfReader(io.BytesIO(contents))\n",
        "                    text = \"\"\n",
        "                    for page in pdf_reader.pages:\n",
        "                        text += page.extract_text()\n",
        "                    combined_text.append(text)\n",
        "                    fnames.append(file.name)\n",
        "                # Text file\n",
        "                elif file.type == 'text/plain':\n",
        "                    combined_text.append(contents.decode())\n",
        "                    fnames.append(file.name)\n",
        "                else:\n",
        "                    st.warning(\"Unsupported file type for file: {}\".format(file.name))\n",
        "            text = \" \".join(combined_text)\n",
        "            text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \" \", \"\"], chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "            texts = text_splitter.split_text(text)\n",
        "            chunk_ids = [{\"chunk_id\": i} for i in range(len(texts))]\n",
        "            chunks = text_splitter.create_documents(texts, chunk_ids)\n",
        "            prompt = ANSWER_PROMPT_DEFAULT\n",
        "\n",
        "        # Get LLM\n",
        "        if 'llm' not in st.session_state:\n",
        "            st.session_state.llm = get_llm(model)\n",
        "\n",
        "        # Get retriver\n",
        "        if 'retriever' not in st.session_state:\n",
        "            st.session_state.retriever = get_retriever(chunks, retriever_type, embeddings, num_neighbors, st.session_state.llm, hybrid_weight)\n",
        "\n",
        "if 'llm' in st.session_state:\n",
        "    memory = ConversationBufferWindowMemory(k=4, memory_key=\"chat_history\", input_key=\"question\", output_key=\"answer\", return_messages=True)\n",
        "\n",
        "    if \"chat_history\" not in st.session_state:\n",
        "        st.session_state.chat_history = []\n",
        "    else:\n",
        "        for chat in st.session_state.chat_history:\n",
        "            memory.save_context({'question': chat['human']}, {'answer': chat['AI']})\n",
        "\n",
        "    chain = CustomConversationalRetrievalChain.from_llm(\n",
        "          llm=st.session_state.llm,\n",
        "          memory=memory,\n",
        "          combine_docs_chain_kwargs={\"prompt\": prompt},\n",
        "          condense_question_prompt=QUESTION_GENERATOR_PROMPT,\n",
        "          retriever=st.session_state.retriever,\n",
        "          reranker_type=reranker_type,\n",
        "          condense_question_llm=st.session_state.llm,\n",
        "          chain_type=\"stuff\",\n",
        "          output_key=\"answer\",\n",
        "          response_if_no_docs_found=\"I am sorry, I don't have available information to answer that question.\",\n",
        "          verbose=True,\n",
        "          return_generated_question=True,\n",
        "          return_source_documents=True,\n",
        "          )\n",
        "\n",
        "    if \"user_input\" not in st.session_state:\n",
        "        st.session_state.user_input = \"\"\n",
        "\n",
        "    def submit():\n",
        "        st.session_state.user_input = st.session_state.widget\n",
        "        st.session_state.widget = \"\"\n",
        "\n",
        "    st.text_input(\"You: \",\n",
        "                  key=\"widget\",\n",
        "                  placeholder=\"Hello, I am your AI assistant! How can I help you?\",\n",
        "                  label_visibility='hidden',\n",
        "                  on_change=submit)\n",
        "\n",
        "    user_input = st.session_state.user_input\n",
        "\n",
        "    if user_input:\n",
        "        with st.spinner(\"Generating answer...\"):\n",
        "            output = chain.invoke(input=user_input)\n",
        "            answer = output[\"answer\"]\n",
        "            sources = output.get(\"source_documents\", [])\n",
        "\n",
        "        # Only show sources for the current (last) output\n",
        "        if sources:\n",
        "            with st.expander(\"Sources used in Answer\", expanded=False):\n",
        "                for i, source in enumerate(sources, start=1):\n",
        "                    st.markdown(f\"**Source {i}:**\\n{source.page_content}\")\n",
        "\n",
        "        st.session_state.chat_history.append({'human': user_input, 'AI': answer})\n",
        "\n",
        "    messages = st.session_state.get(\"chat_history\", [])\n",
        "    # Display messages in reverse order\n",
        "    for msg in reversed(messages):\n",
        "        if 'human' in msg and msg['human']:\n",
        "            message(msg['human'], is_user=True, key=str(msg) + \"_user\")\n",
        "        if 'AI' in msg and msg['AI']:\n",
        "            message(msg['AI'], is_user=False, key=str(msg) + \"_ai\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-zWW5YjyEjw"
      },
      "source": [
        "# Run App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-t88xvhDwXyJ",
        "outputId": "badb6fd8-7b9f-4e21-96e0-3cdbb8292978"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.124.215.20\n"
          ]
        }
      ],
      "source": [
        "!streamlit run chatbot.py &>/content/logs.txt & curl ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH4PRVTBwblK",
        "outputId": "a07c031d-fce3-4f1b-b2e5-4118733bb246"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.806s\n",
            "your url is: https://many-dancers-sin.loca.lt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHdExm5ZtuR7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "aoRDpFxvxHup"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}