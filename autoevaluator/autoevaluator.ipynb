{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQwFw7Bqy_Fw"
      },
      "source": [
        "# Installs and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI9dTQU1y-l9",
        "outputId": "868570be-dcfc-4cae-baa4-abfe0bfadd24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.7/806.7 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.0/509.0 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.1/248.1 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.1/149.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.2/320.2 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.0/237.0 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.7/60.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.6/105.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for llama-index (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "bigframes 0.20.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 1.4.3 which is incompatible.\n",
            "bigframes 0.20.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.2.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 1.4.3 which is incompatible.\n",
            "plotnine 0.12.4 requires pandas>=1.5.0, but you have pandas 1.4.3 which is incompatible.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "pandas==1.4.3 \\\n",
        "langchain \\\n",
        "python-multipart \\\n",
        "InstructorEmbedding \\\n",
        "chromadb \\\n",
        "uvicorn==0.18.3 \\\n",
        "openai==0.27.0 \\\n",
        "tiktoken==0.3.1 \\\n",
        "huggingface-hub \\\n",
        "pypdf==3.7.1 \\\n",
        "filetype==1.2.0 \\\n",
        "tokenizers==0.14.0 \\\n",
        "sentence-transformers==2.2.2 \\\n",
        "scikit-learn==1.2.1 \\\n",
        "llama-index==0.4.35 \\\n",
        "sse_starlette==1.3.3 \\\n",
        "gpt-index==0.5.16 \\\n",
        "python-dotenv==1.0.0 \\\n",
        "rank_bm25 \\\n",
        "streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQcyELPvaOvp",
        "outputId": "d8cc9eb3-cb89-4917-fbfa-dd846bc1b39e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.38.tar.gz (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.9.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.4)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.38-cp310-cp310-manylinux_2_35_x86_64.whl size=9660741 sha256=87a8364bc97ae0415d9a62b58b9a3c14f56a3e87dac11abb32ef7459849c2280\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/58/77/20d3d9a235b4930050fbcde1ad4f0a4d054644269e801b08aa\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.2.38\n"
          ]
        }
      ],
      "source": [
        "# get GPU to run LLLM\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI7ovBIa0X4p",
        "outputId": "988dab77-8f3c-4742-bb2d-da1e44a3a56e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Path to your PCB folder in Google Drive\n",
        "DRIVE_FOLDER = '/content/drive/My Drive/PCB/EN_docs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sVt98bVRjXS"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aMJhQUvu5M8",
        "outputId": "08d5d908-c4a1-434a-8f7e-2abecc7c04aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device:\", device)\n",
        "if device == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuTCqL7AhJ-U"
      },
      "source": [
        "# auxiliary.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We2kbJyAhNXD",
        "outputId": "88765eb7-ed85-4715-c933-41a0a21b63c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting auxiliary.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile auxiliary.py\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.vectorstores import VectorStoreRetriever\n",
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain_core.documents.base import Document\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "########################\n",
        "# MultiQuery Retriever #\n",
        "########################\n",
        "\n",
        "# Output parser will split the LLM result into a list of queries\n",
        "class LineList(BaseModel):\n",
        "    # \"lines\" is the key (attribute name) of the parsed output\n",
        "    lines: List[str] = Field(description=\"Lines of text\")\n",
        "\n",
        "\n",
        "class LineListOutputParser(PydanticOutputParser):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__(pydantic_object=LineList)\n",
        "\n",
        "    def parse(self, text: str) -> LineList:\n",
        "        lines = text.strip().split(\"\\n\")\n",
        "        return LineList(lines=lines)\n",
        "\n",
        "\n",
        "output_parser = LineListOutputParser()\n",
        "\n",
        "MULTIQUERY_TEMPLATE = \"\"\"<s>[INST] You are an AI language model assistant.\n",
        "Your task is to generate 3 different search queries that aim to answer the user question from multiple perspectives.\n",
        "The user questions are focused on Banking, Finance, and related disciplines.\n",
        "Each query MUST tackle the question from a different viewpoint, we want to get a variety of RELEVANT search results.\n",
        "Provide these alternative questions separated by newlines. [/INST]\n",
        "\n",
        "Original question: {question}\n",
        "\"\"\"\n",
        "\n",
        "MULTIQUERY_PROMPT = PromptTemplate(\n",
        "    input_variables=['question'],\n",
        "    template=MULTIQUERY_TEMPLATE,\n",
        "    )\n",
        "\n",
        "\n",
        "####################################\n",
        "# Expansion with Answer Generation #\n",
        "####################################\n",
        "\n",
        "QUERY_EXPANSION_TEMPLATE = \"\"\"<s>[INST]\n",
        "Offer a concise and factual answer to the question. The response should be brief and without conversational tone, empathy, greetings, or personal comments.\n",
        "[/INST]\n",
        "\n",
        "QUESTION: {new_question}\n",
        "\n",
        "Possible Answer:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "QUERY_EXPANSION_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"new_question\"],\n",
        "    template=QUERY_EXPANSION_TEMPLATE,\n",
        ")\n",
        "\n",
        "# Retriever that processes the initial query through an LLM Chain\n",
        "class CustomRetriever(VectorStoreRetriever):\n",
        "    chain: LLMChain\n",
        "    vectorstore: VectorStoreRetriever\n",
        "    search_kwargs: dict = Field(default_factory=dict)\n",
        "\n",
        "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
        "        output = self.chain.invoke(query)\n",
        "        new_query = output['new_question'] + \"\\n\" + output['query']\n",
        "        results = self.vectorstore.get_relevant_documents(query=new_query)\n",
        "        return results\n",
        "\n",
        "######################\n",
        "# Evaluate Answering #\n",
        "######################\n",
        "\n",
        "binary_answer_template = \"\"\"<s>[INST] Your task is to evaluate the quality of our generated answer compared to a reference answer for a given query.\n",
        "Please structure your response as follows:\n",
        "- Start with \"Score:\" followed by a numerical score of EITHER 0 or 1. Use 0 if the generated answer is incorrect, and 1 if it is correct.\n",
        "- On a new line, start with \"Reasoning:\" and provide your reasoning for the score given. Make sure to explain why the generated answer is correct or incorrect in relation to the query and reference answer.\n",
        "\n",
        "Your adherence to this response format is crucial for accurate assessment.\n",
        "[/INST]\n",
        "\n",
        "QUERY: {query}\n",
        "\n",
        "GENERATED ANSWER: {result}\n",
        "\n",
        "REFERENCE ANSWER: {answer}\n",
        "\n",
        "RESPONSE:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "BINARY_ANSWER_PROMPT = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=binary_answer_template)\n",
        "\n",
        "\n",
        "score_answer_template = \"\"\"<s>[INST] Your task is to evaluate the quality of our generated answer compared to a reference answer for a given query.\n",
        "Please structure your response as follows:\n",
        "- Start with \"Score:\" followed by a numerical score between 0 and 4. Use ONLY integer values (0, 1, 2 or 4).\n",
        "- On a new line, start with \"Reasoning:\" and provide your reasoning for the score given. Make sure to explain why the generated answer is correct or incorrect in relation to the query and reference answer.\n",
        "\n",
        "Your adherence to this response format is crucial for accurate assessment.\n",
        "[/INST]\n",
        "\n",
        "QUERY: {query}\n",
        "\n",
        "GENERATED ANSWER: {result}\n",
        "\n",
        "REFERENCE ANSWER: {answer}\n",
        "\n",
        "RESPONSE:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "SCORE_ANSWER_PROMPT = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=score_answer_template)\n",
        "\n",
        "############################\n",
        "# ChatBot Answering Prompt #\n",
        "############################\n",
        "\n",
        "template = \"\"\"<s>[INST] Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum. Keep the answer as concise as possible. [/INST]\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Helpful Answer:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFJ_zRxOzZE2"
      },
      "source": [
        "# auto-evaluator.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_CpIrDVyuoV",
        "outputId": "fffa913c-2e63-4ec1-b75d-65328294923e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import io\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "\n",
        "import altair as alt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pypdf\n",
        "import streamlit as st\n",
        "import tiktoken\n",
        "from sentence_transformers import CrossEncoder\n",
        "from transformers import (AutoTokenizer, AutoModelForCausalLM, AutoConfig,\n",
        "                          BitsAndBytesConfig, pipeline)\n",
        "\n",
        "from auxiliary import device, CustomRetriever, QUERY_EXPANSION_PROMPT, LineListOutputParser, LineList, MULTIQUERY_PROMPT, BINARY_ANSWER_PROMPT, SCORE_ANSWER_PROMPT, QA_CHAIN_PROMPT\n",
        "from langchain.chains import QAGenerationChain, RetrievalQA, LLMChain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.evaluation.qa import QAEvalChain, QAGenerateChain\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings, LlamaCppEmbeddings, OpenAIEmbeddings\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever, MultiQueryRetriever\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_community.document_transformers import LongContextReorder\n",
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain.output_parsers import RegexParser\n",
        "from chromadb.errors import InvalidDimensionException\n",
        "\n",
        "\n",
        "st.set_page_config(page_title='🔍 ChatBot Evaluation Tool 🛠️', layout='wide')\n",
        "\n",
        "# Define the columns for the aggregate results table\n",
        "aggregate_results_columns = ['answer model', 'retriever', 'embedding', 'chunk size', 'chunk overlap', 'k docs retrieved', 'reranker', 'evaluation model', 'evaluation questions', 'Answer score', 'Precision', 'Recall', 'Hit Rate', 'MRR', 'AP', 'NDCG', 'Latency']\n",
        "\n",
        "\n",
        "if \"existing_df\" not in st.session_state:\n",
        "    st.session_state.existing_df = pd.DataFrame(columns=aggregate_results_columns)\n",
        "else:\n",
        "    # If it exists, use the existing DataFrame\n",
        "    summary = st.session_state.existing_df\n",
        "\n",
        "\n",
        "def read_json_file(file_path):\n",
        "    \"\"\"\n",
        "    Reads a JSON file and returns its contents.\n",
        "    @param file_path: The path of the file to read.\n",
        "    @return: The contents of the JSON file or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            return json.load(file)\n",
        "    except IOError as e:\n",
        "        st.error(f\"Error reading file {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def enrich_document(entry, doc_id):\n",
        "    \"\"\"\n",
        "    Creates and enriches a Document object from a JSON entry.\n",
        "    @param entry: The JSON entry containing document data.\n",
        "    @param doc_id: The ID to assign to the document.\n",
        "    @return: The enriched Document object or None if entry is invalid.\n",
        "    \"\"\"\n",
        "    answer = entry.get('answer', '')\n",
        "    if answer == '':\n",
        "        return None\n",
        "    source = entry.get('link', '')\n",
        "    topic = entry.get('question') or entry.get('question_topic', '')\n",
        "    if source == \"https://clientebancario.bportugal.pt/en/perguntas-frequentes\":\n",
        "        topic_for_content = topic.split(\" -> \")[-1]\n",
        "        page_content = f\"{topic_for_content}\\n\\n{answer}\"\n",
        "    else:\n",
        "        page_content = f\"{topic}\\n\\n{answer}\"\n",
        "    return Document(page_content=page_content, metadata={\"source\": source, \"topic\": topic, \"doc_id\": doc_id})\n",
        "\n",
        "def update_level_2_documents(documents):\n",
        "    \"\"\"\n",
        "    Updates level 2 documents with corresponding level 3 topics.\n",
        "    @param documents: A list of Document objects.\n",
        "    @return: None. The function modifies the documents list in place.\n",
        "    \"\"\"\n",
        "    level_2_docs = {}\n",
        "    level_3_topics = []\n",
        "\n",
        "    # Categorize documents based on their level\n",
        "    for doc in documents:\n",
        "        topic = doc.metadata.get('topic', '')\n",
        "        level = topic.count('->') + 1\n",
        "        if level == 2:\n",
        "            level_2_docs[topic] = doc\n",
        "        elif level == 3:\n",
        "            level_3_topics.append(topic)\n",
        "\n",
        "    # Update level 2 documents if they have corresponding level 3 topics\n",
        "    for level_2_topic, level_2_doc in level_2_docs.items():\n",
        "        if level_2_doc.page_content.endswith(':'):\n",
        "            # Extract the base of the level 2 topic for matching with level 3 topics\n",
        "            base_topic = level_2_topic + ' -> '\n",
        "            matching_topics = [t.split(' -> ')[-1] for t in level_3_topics if t.startswith(base_topic)]\n",
        "            if matching_topics:\n",
        "                level_2_doc.page_content += '\\n' + '\\n'.join(matching_topics)\n",
        "\n",
        "def get_documents(folder_path=\"/content/drive/My Drive/PCB/EN_docs\"):\n",
        "    \"\"\"\n",
        "    Loads and processes documents from a specified folder.\n",
        "    @param folder_path: The path of the folder containing the documents.\n",
        "    @return: A list of processed Document objects.\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "    doc_id = 0\n",
        "\n",
        "    for file_name in sorted(os.listdir(folder_path)):\n",
        "        if file_name.endswith('.json'):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "            json_data = read_json_file(file_path)\n",
        "            if json_data:\n",
        "                for entry in json_data.get('content', []):\n",
        "                    doc = enrich_document(entry, doc_id)\n",
        "                    if doc:\n",
        "                        documents.append(doc)\n",
        "                        doc_id += 1\n",
        "\n",
        "    # Update level 2 documents with corresponding level 3 topics\n",
        "    update_level_2_documents(documents)\n",
        "\n",
        "    return documents\n",
        "\n",
        "\n",
        "def token_len(text):\n",
        "    tokenizer = tiktoken.get_encoding('cl100k_base')\n",
        "    tokens = tokenizer.encode(\n",
        "        text,\n",
        "        disallowed_special=()\n",
        "    )\n",
        "    return len(tokens)\n",
        "\n",
        "def get_chunks(documents, chunk_size, overlap):\n",
        "    \"\"\"\n",
        "    Split documents into chunks\n",
        "    @param text: documents to split\n",
        "    @param chunk_size: charecters per split\n",
        "    @param overlap: charecter overlap between chunks\n",
        "    @return: list of chunks\n",
        "    \"\"\"\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        "                                                chunk_size=chunk_size,\n",
        "                                                chunk_overlap=overlap,\n",
        "                                                length_function=token_len)\n",
        "    chunks = []\n",
        "    chunk_id = 0\n",
        "    for doc in documents:\n",
        "        page_content = doc.page_content\n",
        "        metadata = doc.metadata\n",
        "        chunked_doc = text_splitter.create_documents(\n",
        "            texts=[page_content],\n",
        "            metadatas=[metadata]\n",
        "        )\n",
        "        for chunked_text in chunked_doc:\n",
        "            chunked_metadata = chunked_text.metadata\n",
        "\n",
        "            # remove chunks that only have the topic or the FAQ\n",
        "            if chunked_text.metadata['source'] == \"https://clientebancario.bportugal.pt/en/perguntas-frequentes\":\n",
        "                topic = chunked_text.metadata['topic'].split(\" -> \")[-1]\n",
        "            else:\n",
        "                topic = chunked_text.metadata['topic']\n",
        "\n",
        "            if chunked_text.page_content.strip() == topic.strip():\n",
        "                continue\n",
        "\n",
        "            # Add the chunk_id to the metadata\n",
        "            chunked_metadata['chunk_id'] = chunk_id\n",
        "            new_chunk = Document(page_content=chunked_text.page_content, metadata=chunked_metadata)\n",
        "            chunks.append(new_chunk)\n",
        "            chunk_id += 1\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def load_mistral(mistral_model):\n",
        "    \"\"\"\n",
        "    Load mistral model from path\n",
        "    @mistral_model: mistral model to load\n",
        "    \"\"\"\n",
        "\n",
        "    if mistral_model == \"Mistral-7B-Instruct-v0.1 Small\":\n",
        "        model_path = 'mistral-7b-instruct-v0.1.Q2_K.gguf'\n",
        "\n",
        "    elif mistral_model == \"Mistral-7B-Instruct-v0.1 Med\":\n",
        "        model_path = 'mistral-7b-instruct-v0.1.Q4_K_M.gguf'\n",
        "\n",
        "    llm = LlamaCpp(\n",
        "          model_path=f\"/content/drive/My Drive/PCB/mistral/{model_path}\",\n",
        "          temperature=0.0,\n",
        "          f16_kv=True,\n",
        "          top_p=1,\n",
        "          n_ctx=4096,\n",
        "          n_batch=1024,\n",
        "          n_gpu_layers=100)\n",
        "\n",
        "    return llm\n",
        "\n",
        "@st.cache_resource\n",
        "def get_llm(model):\n",
        "    \"\"\"\n",
        "    Get LLM\n",
        "    @param model: LLM to use\n",
        "    @return: LLM\n",
        "    \"\"\"\n",
        "\n",
        "    if model == 'GPT-4-turbo':\n",
        "        llm = ChatOpenAI(model_name=\"gpt-4-0125-preview\", temperature=0)\n",
        "\n",
        "    elif model == \"Mistral-7B-Instruct-v0.1 Small\" or model == \"Mistral-7B-Instruct-v0.1 Med\":\n",
        "        llm = load_mistral(model)\n",
        "\n",
        "    return llm\n",
        "\n",
        "\n",
        "def get_retriever(chunks, retriever_type, embedding_type, num_neighbors, llm, hybrid_weight):\n",
        "    \"\"\"\n",
        "    Get document retriever\n",
        "    @param chunks: list of Documents (chunks)\n",
        "    @param retriever_type: retriever type\n",
        "    @param embedding_type: embedding type\n",
        "    @param num_neighbors: number of neighbors for retrieval\n",
        "    @return: retriever\n",
        "    \"\"\"\n",
        "\n",
        "    if embedding_type == \"OpenAI\":\n",
        "        embd = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "    elif embedding_type in [\"bge-base-en-v1.5\", \"bge-small-en-v1.5\", \"bge-large-en-v1.5\"]:\n",
        "        model_name = f\"BAAI/{embedding_type}\"\n",
        "        embd = HuggingFaceInstructEmbeddings(model_name=model_name, model_kwargs={\"device\": device})\n",
        "\n",
        "    if retriever_type == \"Similarity\":\n",
        "        try:\n",
        "            vectorstore = Chroma.from_documents(chunks,  embd)\n",
        "        except (InvalidDimensionException, IndexError):\n",
        "            Chroma().delete_collection()\n",
        "            vectorstore = Chroma.from_documents(chunks, embd)\n",
        "        retriever = vectorstore.as_retriever(search_type='similarity_score_threshold', search_kwargs={\"k\": num_neighbors, \"score_threshold\": 0.4})\n",
        "\n",
        "    elif retriever_type == \"BM25\":\n",
        "        retriever = BM25Retriever.from_documents(chunks)\n",
        "\n",
        "    elif retriever_type == \"Hybrid-search\":\n",
        "        try:\n",
        "            vectorstore = Chroma.from_documents(chunks, embd)\n",
        "        except (InvalidDimensionException, IndexError):\n",
        "            Chroma().delete_collection()\n",
        "            vectorstore = Chroma.from_documents(chunks, embd)\n",
        "        chroma_retriever = vectorstore.as_retriever(search_type='similarity_score_threshold', search_kwargs={\"k\": num_neighbors, \"score_threshold\": 0.4})\n",
        "        bm25_retriever = BM25Retriever.from_documents(chunks)\n",
        "        retriever = EnsembleRetriever(retrievers=[bm25_retriever, chroma_retriever],weights=[hybrid_weight,1-hybrid_weight])\n",
        "\n",
        "    elif retriever_type == \"Similarity + Expansion w/ Generated Answers\":\n",
        "        try:\n",
        "            vectorstore = Chroma.from_documents(chunks, embd)\n",
        "        except (InvalidDimensionException, IndexError):\n",
        "            Chroma().delete_collection()\n",
        "            vectorstore = Chroma.from_documents(chunks, embd)\n",
        "        chain = LLMChain(llm=llm, prompt=QUERY_EXPANSION_PROMPT, output_key=\"query\")\n",
        "        retriever = CustomRetriever(vectorstore=vectorstore.as_retriever(search_type='similarity_score_threshold', search_kwargs={\"k\": num_neighbors, \"score_threshold\": 0.4}), chain=chain)\n",
        "        mq_retrieval_chain = LLMChain(llm=llm, prompt=MULTIQUERY_PROMPT, output_parser=LineListOutputParser())\n",
        "\n",
        "    elif retriever_type == \"MultiQuery Retriever\":\n",
        "        try:\n",
        "            vectorstore = Chroma.from_documents(chunks, embd)\n",
        "        except (InvalidDimensionException, IndexError):\n",
        "            Chroma().delete_collection()\n",
        "            vectorstore = Chroma.from_documents(chunks, embd)\n",
        "        chain = LLMChain(llm=llm, prompt=MULTIQUERY_PROMPT, output_parser=LineListOutputParser())\n",
        "        retriever = MultiQueryRetriever(\n",
        "                    retriever=vectorstore.as_retriever(search_type='similarity_score_threshold', search_kwargs={\"k\": num_neighbors, \"score_threshold\": 0.4}),\n",
        "                    llm_chain=mq_retrieval_chain,\n",
        "                    parser_key=\"lines\",\n",
        "                    include_original=True\n",
        "                    )\n",
        "\n",
        "    return retriever\n",
        "\n",
        "def reranking(reranker_type, retrieved_docs, question):\n",
        "    \"\"\"\n",
        "    Rerank retrieved docs\n",
        "    @param reranker_type: reranker model to be used\n",
        "    @param retrieved_docs: documents coming from the retrieval step to be reranked\n",
        "    @param question: question used on the retrieval step\n",
        "    @return: retriever\n",
        "    \"\"\"\n",
        "    reranked_docs = retrieved_docs\n",
        "\n",
        "    if reranker_type in [\"ms-marco-MiniLM-L-6-v2\", \"bge-reranker-base\"]:\n",
        "        model_name = 'cross-encoder/ms-marco-MiniLM-L-6-v2' if reranker_type == \"ms-marco-MiniLM-L-6-v2\" else 'BAAI/bge-reranker-base'\n",
        "        cross_encoder = CrossEncoder(model_name, device=device)\n",
        "        scores = cross_encoder.predict([[question, doc.page_content] for doc in retrieved_docs])\n",
        "        reranked_docs = [doc for doc, score in sorted(zip(retrieved_docs, scores), key=lambda x: x[1], reverse=True)]\n",
        "\n",
        "    elif reranker_type == \"LongContextReorder\":\n",
        "        reranker = LongContextReorder()\n",
        "        reranked_docs = reranker.transform_documents(retrieved_docs)\n",
        "\n",
        "    return reranked_docs\n",
        "\n",
        "def generate_eval(chunks, llm, num_questions):\n",
        "    \"\"\"\n",
        "    Generate multiple question/answer pairs from random chunks based on chunk_id.\n",
        "    @param chunks: list of chunks with chunk_id\n",
        "    @param num_questions: number of question/answer pairs to generate\n",
        "    @return: list of dicts, each with keys \"question\", \"answer\" and \"chunk_id\"\n",
        "    \"\"\"\n",
        "\n",
        "    qa_gen_chain = QAGenerateChain.from_llm(llm, output_parser=RegexParser(regex='QUESTION: (.*?)\\\\n+ANSWER: (.*)', output_keys=['question', 'answer']))\n",
        "    eval_pairs = []\n",
        "\n",
        "    for _ in range(num_questions):\n",
        "        random_chunk = random.choice(chunks)\n",
        "        chunk_text = random_chunk.page_content\n",
        "        awaiting_answer = True\n",
        "        while awaiting_answer:\n",
        "            try:\n",
        "                output = qa_gen_chain.invoke(chunk_text)\n",
        "                qa_pair = output['qa_pairs']\n",
        "                qa_pair['sources'] = [random_chunk.metadata['chunk_id']]\n",
        "                eval_pairs.append(qa_pair)\n",
        "                awaiting_answer = False\n",
        "            except:\n",
        "                st.error(\"Error on question\")\n",
        "                random_chunk = random.choice(chunks)\n",
        "                chunk_text = random_chunk.page_content\n",
        "\n",
        "    return eval_pairs\n",
        "\n",
        "def grade_model_answer(gt_dataset, predictions, model_eval, grade_answer_prompt):\n",
        "    \"\"\"\n",
        "    Grades the answer based on ground truth and model predictions.\n",
        "    @param gt_dataset: list of dictionaries containing ground truth questions and answers.\n",
        "    @param predictions: A list of dictionaries containing model predictions for the questions.\n",
        "    @model_eval: Model used to evaluate the answers & retrieval\n",
        "    @param grade_answer_prompt: The prompt for the grading. Either \"Binary\" or \"Score\".\n",
        "    @return answers_grade: A list of strings - scores + reasoning for the generated answers.\n",
        "    \"\"\"\n",
        "\n",
        "    if grade_answer_prompt == \"Binary\":\n",
        "        prompt = BINARY_ANSWER_PROMPT\n",
        "    elif grade_answer_prompt == \"Score\":\n",
        "        prompt = SCORE_ANSWER_PROMPT\n",
        "\n",
        "    if model_eval == 'GPT-4-turbo':\n",
        "        eval_chain = QAEvalChain.from_llm(llm=ChatOpenAI(model_name=\"gpt-4-0125-preview\", temperature=0),\n",
        "                                          prompt=prompt)\n",
        "\n",
        "    elif model == \"Mistral-7B-Instruct-v0.1 Small\" or model == \"Mistral-7B-Instruct-v0.1 Med\":\n",
        "        llm = load_mistral(model)\n",
        "        eval_chain = QAEvalChain.from_llm(llm=llm, prompt=prompt)\n",
        "\n",
        "    answers_grade = eval_chain.evaluate(gt_dataset,\n",
        "                                         predictions,\n",
        "                                         question_key=\"question\",\n",
        "                                         prediction_key=\"result\")\n",
        "    return answers_grade\n",
        "\n",
        "def grade_model_retrieval(gt_dataset, retrieved_docs):\n",
        "    \"\"\"\n",
        "    Calculate Retrieval Metrics for each question in the ground truth dataset.\n",
        "\n",
        "    @param gt_dataset: Ground truth dataset, a list of dicts with 'question' and 'sources' (correct chunk_id sources for the question).\n",
        "    @param retrieved_docs: Retrieved documents, a list of dicts with 'question' and 'reranked_doc_ids' (list of reranked chunk_ids for the question).\n",
        "\n",
        "    @return retrieval_metrics: A list of dicts with the retrieval metrics for each question, including precision, recall, hit rate, MRR, AP, and NDCG.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert gt_dataset into a more accessible format\n",
        "    truth_dict = {item['question']: set(item['sources']) for item in gt_dataset}\n",
        "\n",
        "    retrieval_metrics = []\n",
        "\n",
        "    # Helper function to calculate NDCG\n",
        "    def ndcg_at_k(relevance_scores, num_retrieved_docs, num_relevant_docs):\n",
        "        r = np.asfarray(relevance_scores)[:num_retrieved_docs]\n",
        "        if r.size:\n",
        "            # Calculate DCG\n",
        "            log_base = np.log2(np.arange(2, r.size + 2))\n",
        "            dcg = np.sum(r / log_base)\n",
        "\n",
        "            # Calculate IDCG\n",
        "            ideal_relevance_scores = np.ones(min(num_relevant_docs, num_retrieved_docs))\n",
        "            idcg_log_base = np.log2(np.arange(2, min(num_relevant_docs, num_retrieved_docs) + 2))\n",
        "            idcg = np.sum(ideal_relevance_scores / idcg_log_base)\n",
        "\n",
        "            # Calculate NDCG\n",
        "            ndcg_score = dcg / idcg if idcg else 0\n",
        "\n",
        "            # Debugging output\n",
        "            #st.write(\"DCG:\", dcg, \"IDCG:\", idcg, \"NDCG:\", ndcg_score)\n",
        "            return ndcg_score\n",
        "        return 0\n",
        "\n",
        "    for doc in retrieved_docs:\n",
        "        question = doc['question']\n",
        "        reranked_doc_ids = doc['reranked_doc_ids']\n",
        "        true_chunk_ids = truth_dict.get(question, set())\n",
        "\n",
        "        tp = len(true_chunk_ids.intersection(reranked_doc_ids))\n",
        "        fp = len(set(reranked_doc_ids) - true_chunk_ids)\n",
        "        fn = len(true_chunk_ids - set(reranked_doc_ids))\n",
        "\n",
        "        precision = tp / (tp + fp) if tp + fp else 0\n",
        "        recall = tp / (tp + fn) if tp + fn else 0\n",
        "        hit_rate = 1 if tp else 0\n",
        "\n",
        "        relevant_ranks = [1/(i+1) for i, chunk_id in enumerate(reranked_doc_ids) if chunk_id in true_chunk_ids]\n",
        "        mrr = max(relevant_ranks, default=0)\n",
        "\n",
        "        cum_tp = 0\n",
        "        precisions = []\n",
        "        for i, chunk_id in enumerate(reranked_doc_ids):\n",
        "            if chunk_id in true_chunk_ids:\n",
        "                cum_tp += 1\n",
        "                precisions.append(cum_tp / (i + 1))\n",
        "        ap = np.mean(precisions) if precisions else 0\n",
        "\n",
        "        relevance_scores = [1 if chunk_id in true_chunk_ids else 0 for chunk_id in reranked_doc_ids]\n",
        "        num_retrieved_docs = len(reranked_doc_ids)\n",
        "        num_relevant_docs = len(true_chunk_ids)\n",
        "        ndcg = ndcg_at_k(relevance_scores, num_retrieved_docs, num_relevant_docs)\n",
        "\n",
        "        retrieval_metrics.append({\n",
        "            \"question\": question,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"Hit Rate\": hit_rate,\n",
        "            \"MRR\": mrr,\n",
        "            \"AP\": ap,\n",
        "            \"NDCG\": ndcg\n",
        "        })\n",
        "\n",
        "    return retrieval_metrics\n",
        "\n",
        "def run_evaluation(chain, retriever, gt_dataset, model_eval, grade_prompt, text):\n",
        "    \"\"\"\n",
        "    Runs evaluation on a model's performance on a given evaluation dataset.\n",
        "    @param chain: Model chain used for answering questions\n",
        "    @param retriever:  Document retriever used for retrieving relevant documents\n",
        "    @param eval_set: List of dictionaries containing questions and corresponding ground truth answers\n",
        "    @param model_eval: Model used to grade the answers & retrieval\n",
        "    @param grade_prompt: String prompt used for grading model's performance\n",
        "    @return: A tuple of four items:\n",
        "    - answers_grade: A dictionary containing scores for the model's answers.\n",
        "    - retrieval_metrics: A list of lists with the retrieval metrics for each question.\n",
        "    - latencies_list: A list of latencies in seconds for each question answered.\n",
        "    - predictions_list: A list of dictionaries containing the model's predicted answers and relevant documents for each question.\n",
        "    \"\"\"\n",
        "\n",
        "    predictions = []\n",
        "    retrieved_docs = []\n",
        "    latencies_list = []\n",
        "\n",
        "    for eval_qa_pair in gt_dataset:\n",
        "        # Get answer and log latency\n",
        "        start_time = time.time()\n",
        "        question = eval_qa_pair[\"question\"]\n",
        "\n",
        "        # Retrieved docs\n",
        "        initial_docs = retriever.get_relevant_documents(question)\n",
        "        docs = reranking(reranker_type, initial_docs, question)\n",
        "        reranked_doc_ids = [doc.metadata['chunk_id'] for doc in docs]\n",
        "        retrieved_docs.append({\"question\": question, \"reranked_doc_ids\": reranked_doc_ids})\n",
        "\n",
        "        # Prediction\n",
        "        context = '\\n\\n'.join([doc.page_content for doc in docs])\n",
        "        result = chain.invoke({'question': question, 'context': context})\n",
        "        predictions.append({\"question\": question, \"answer\": eval_qa_pair[\"answer\"], \"result\": result['text']})\n",
        "\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        latencies_list.append(elapsed_time)\n",
        "\n",
        "    # Grading\n",
        "    answers_grade = grade_model_answer(gt_dataset, predictions, model_eval, grade_prompt)\n",
        "    retrieval_metrics = grade_model_retrieval(gt_dataset, retrieved_docs)\n",
        "\n",
        "    return answers_grade, retrieval_metrics, latencies_list, predictions\n",
        "\n",
        "\n",
        "def generate_url(oai_api_key, chunk_size, chunk_overlap, embeddings, retriever_type, hybrid_weight, num_neighbors, reranker_type, model):\n",
        "    base_url = \"https://tiny-bars-wink.loca.lt\"\n",
        "    return f\"{base_url}/?oai_api_key={oai_api_key.strip()}&chunk_size={chunk_size}&chunk_overlap={chunk_overlap}&retriever_type={retriever_type}&num_neighbors={num_neighbors}&hybrid_weight={hybrid_weight}&embeddings={embeddings}&reranker_type={reranker_type}&model={model}\"\n",
        "\n",
        "# App Design\n",
        "st.info(\"\"\"\n",
        "    # 🤖 Q&A Chatbot Evaluation Tool\n",
        "\n",
        "    **This is an evaluation tool for question-answering chatbots.** 💡\n",
        "\n",
        "    - 📝 Create your custom ChatBot for your specific documents.\n",
        "    - 🕵️ Evaluate the RAG pipeline for its quality in retrieval and question answering.\n",
        "    - 🧩 Test with different settings to build your perfect ChatBot assistant.\n",
        "\n",
        "    ## How It Works:\n",
        "    - **Document Selection**: Choose to upload your own documents or use the pre-loaded Bank Customer Website case. 📁\n",
        "    - **Ground Truth Options**: Submit your ground truth for detailed and accurate results, or let the app generate it for convenience. 🎯\n",
        "    - **Compare Different Experiments**: Analyze and compare the results of various experiments to find the best RAG setting. 🔍\n",
        "    - **Launch Your Custom ChatBot**: Find the ideal RAG combination, launch your custom ChatBot and interact with it live! ✅\n",
        "\n",
        "    With this tool, you can **experiment with different configurations** and decide what best suits your data to create your perfect ChatBot! 🚀\n",
        "\"\"\")\n",
        "\n",
        "# ChatBot parameters\n",
        "with st.sidebar.form(\"user_input\"):\n",
        "\n",
        "    oai_api_key = st.text_input(\"`OpenAI API Key:`\", type=\"password\").strip()\n",
        "\n",
        "    chunk_size = st.select_slider(\"`Choose chunk size (in tokens)`\",\n",
        "                                  options=[256, 512, 768, 1024],\n",
        "                                  value=256)\n",
        "\n",
        "    chunk_overlap = st.select_slider(\"`Choose chunk overlap (in tokens)`\",\n",
        "                                    options=[0, 20, 50, 100, 200],\n",
        "                                    value=20)\n",
        "\n",
        "    embeddings = st.radio(\"`Choose embedding model`\",\n",
        "                          (\"bge-base-en-v1.5\", \"bge-small-en-v1.5\", \"bge-large-en-v1.5\", \"OpenAI\"),\n",
        "                          index=0)\n",
        "\n",
        "    retriever_type = st.radio(\"`Choose retriever`\",\n",
        "                              (\"Similarity\", \"Hybrid-search\", \"MultiQuery\", \"Similarity + Expansion w/ Generated Answers\"),\n",
        "                              index=0)\n",
        "\n",
        "    hybrid_weight = st.select_slider(\"`Choose BM25 vs. semantic search weight (0 to 1).\\nOnly active for 'Hybrid-search'.`\",\n",
        "                                    options=[0.00, 0.25, 0.50, 0.75, 1.00],\n",
        "                                    value=0.50)\n",
        "\n",
        "    num_neighbors = st.select_slider(\"`Choose # chunks to retrieve`\",\n",
        "                                    options=[0, 1, 2, 3, 4, 5, 6, 7],\n",
        "                                    value=3)\n",
        "\n",
        "    reranker_type = st.radio(\"`Choose reranker`\",\n",
        "                            (\"LongContextReorder\", \"ms-marco-MiniLM-L-6-v2\", \"bge-reranker-base\", \"None\"),\n",
        "                            index=0)\n",
        "\n",
        "    model = st.radio(\"`Choose ChatBot model`\",\n",
        "                    (\"Mistral-7B-Instruct-v0.1 Small\", \"Mistral-7B-Instruct-v0.1 Med\", \"GPT-4-turbo\"),\n",
        "                    index=0,\n",
        "                    key=\"model\")\n",
        "\n",
        "    model_eval = st.radio(\"`Choose Evaluator model`\",\n",
        "                          (\"Mistral-7B-Instruct-v0.1 Small\", \"Mistral-7B-Instruct-v0.1 Med\", \"GPT-4-turbo\"),\n",
        "                          index=0,\n",
        "                          key=\"model_eval\")\n",
        "\n",
        "    grade_prompt = st.radio(\"`Grading style prompt`\",\n",
        "                            (\"Binary\", \"Score\"),\n",
        "                            index=0)\n",
        "\n",
        "    num_eval_questions = st.slider(\"`Number of test questions`\",\n",
        "                                  min_value=5,\n",
        "                                  max_value=1000,\n",
        "                                  value=10,\n",
        "                                  step=5)\n",
        "\n",
        "    submitted_parameters = st.form_submit_button(\"Submit parameters\")\n",
        "\n",
        "    if 'chatbot_url' not in st.session_state:\n",
        "        st.session_state.chatbot_url = None\n",
        "\n",
        "    if submitted_parameters:\n",
        "        st.session_state.chatbot_url = generate_url(oai_api_key, chunk_size, chunk_overlap, embeddings, retriever_type, hybrid_weight, num_neighbors, reranker_type, model)\n",
        "\n",
        "    if st.session_state.chatbot_url is not None:\n",
        "        # Button to launch the chatbot app with Streamlit style\n",
        "        button_html = f\"\"\"<a href=\"{st.session_state.chatbot_url}\" target=\"_blank\">\n",
        "                            <button style='margin-top: 10px; width: 100%; height: 40px; border: none; border-radius: 20px; background-color: #FF4B4B; color: white;'>\n",
        "                                Launch Custom ChatBot\n",
        "                            </button>\n",
        "                          </a>\"\"\"\n",
        "        st.sidebar.markdown(button_html, unsafe_allow_html=True)\n",
        "\n",
        "# Initialize session state variables if they don't exist\n",
        "if 'document_selected' not in st.session_state:\n",
        "    st.session_state.document_selected = False\n",
        "\n",
        "if 'gt_data_ready' not in st.session_state:\n",
        "    st.session_state.gt_data_ready = False\n",
        "\n",
        "if 'selections_confirmed' not in st.session_state:\n",
        "    st.session_state.selections_confirmed = False\n",
        "\n",
        "if 'uploaded_docs' not in st.session_state:\n",
        "    st.session_state.uploaded_docs = None\n",
        "\n",
        "if 'use_bp_example' not in st.session_state:\n",
        "    st.session_state.use_bp_example = None\n",
        "\n",
        "if 'gt_option' not in st.session_state:\n",
        "    st.session_state.gt_option = None\n",
        "\n",
        "if \"detailed_df\" not in st.session_state:\n",
        "    st.session_state.detailed_df = pd.DataFrame()\n",
        "\n",
        "# Document Upload and Ground Truth Options Section\n",
        "if not st.session_state.selections_confirmed:\n",
        "    # Document Upload Section\n",
        "    st.subheader('Document Upload')\n",
        "    st.session_state.uploaded_docs = st.file_uploader(\"Upload documents (.pdf or .txt):\", type=['pdf', 'txt'], accept_multiple_files=True)\n",
        "    st.session_state.use_bp_example = st.checkbox(\"Use Banco de Portugal - Bank Customer Website\")\n",
        "\n",
        "    # Ground Truth Options Section\n",
        "    st.subheader('Ground Truth Options')\n",
        "    gt_options = ['Generate Ground Truth from Documents', 'Upload Own Ground Truth JSON Document']\n",
        "    if st.session_state.use_bp_example:\n",
        "        gt_options.append('Use pre-loaded Bank Customer Website Ground Truth')\n",
        "    st.session_state.gt_option = st.radio(\"Choose ground truth data handling:\", gt_options)\n",
        "\n",
        "    # Caption with pros and cons of each option\n",
        "    st.caption(\"\"\"\n",
        "    📋 Your Ground Truth must be a JSON file containing a list of dictionaries with the following keys:\n",
        "    - question (your question, a string)\n",
        "    - answer (your reference answer, a string)\n",
        "    - sources (chunk IDs of the chunks used to answer the question, a list of integers).\n",
        "\n",
        "    ⬆️ By uploading your Ground Truth, you can access all retrieval metrics and get more trustworthy results.\n",
        "\n",
        "    🔗 If you upload your Ground Truth, ensure that the chunking used corresponds to the parameters chosen on the sidebar.\n",
        "    \"\"\")\n",
        "\n",
        "    if st.session_state.gt_option == 'Upload Own Ground Truth JSON Document':\n",
        "        uploaded_gt = st.file_uploader(\"Upload Ground Truth JSON file:\", type='json')\n",
        "\n",
        "    submit_selections = st.button('Submit Selections')\n",
        "    if submit_selections:\n",
        "        st.session_state.document_selected = st.session_state.uploaded_docs is not None or st.session_state.use_bp_example\n",
        "        st.session_state.gt_data_ready = st.session_state.gt_option != 'Upload Own Ground Truth JSON Document' or uploaded_gt is not None\n",
        "        st.session_state.selections_confirmed = st.session_state.document_selected and st.session_state.gt_data_ready\n",
        "\n",
        "# Run Experiment Button\n",
        "if st.session_state.selections_confirmed:\n",
        "    st.markdown(\"---\")\n",
        "    run_experiment = st.button('Run Experiment')\n",
        "\n",
        "    if run_experiment:\n",
        "\n",
        "        with st.spinner(\"`Running Experiment...`\"):\n",
        "\n",
        "            if st.session_state.uploaded_docs:\n",
        "                combined_text = []\n",
        "                fnames = []\n",
        "                for file in sorted(st.session_state.uploaded_docs):\n",
        "                    contents = file.read()\n",
        "                    # PDF file\n",
        "                    if file.type == 'application/pdf':\n",
        "                        pdf_reader = pypdf.PdfReader(io.BytesIO(contents))\n",
        "                        text = \"\"\n",
        "                        for page in pdf_reader.pages:\n",
        "                            text += page.extract_text()\n",
        "                        combined_text.append(text)\n",
        "                        fnames.append(file.name)\n",
        "                    # Text file\n",
        "                    elif file.type == 'text/plain':\n",
        "                        combined_text.append(contents.decode())\n",
        "                        fnames.append(file.name)\n",
        "                    else:\n",
        "                        st.warning(\"Unsupported file type for file: {}\".format(file.name))\n",
        "                text = \" \".join(combined_text)\n",
        "                text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \" \", \"\"], chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "                texts = text_splitter.split_text(text)\n",
        "                chunk_ids = [{\"chunk_id\": i} for i in range(len(texts))]\n",
        "                chunks = text_splitter.create_documents(texts, chunk_ids)\n",
        "\n",
        "            elif st.session_state.use_bp_example:\n",
        "                documents = get_documents()\n",
        "                chunks = get_chunks(documents, chunk_size, chunk_overlap)\n",
        "\n",
        "            os.environ[\"OPENAI_API_KEY\"] = oai_api_key\n",
        "            llm = get_llm(model)\n",
        "            retriever = get_retriever(chunks, retriever_type, embeddings, num_neighbors, llm, hybrid_weight)\n",
        "            qa_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)\n",
        "\n",
        "            if st.session_state.gt_option == 'Upload Own Ground Truth JSON Document':\n",
        "                eval_set = json.load(uploaded_gt)\n",
        "\n",
        "            elif st.session_state.gt_option == 'Use pre-loaded Bank Customer Website Ground Truth':\n",
        "                gt_path = '/content/drive/My Drive/PCB/truth_base/gt_dataset.json'\n",
        "                with open(gt_path, 'r') as json_file:\n",
        "                    eval_set = json.load(json_file)\n",
        "\n",
        "            elif st.session_state.gt_option == 'Generate Ground Truth from Documents':\n",
        "                eval_set = generate_eval(chunks, llm, num_eval_questions)\n",
        "\n",
        "            graded_answers, retrieval_metrics, latency, predictions = run_evaluation(qa_chain, retriever, eval_set, model_eval, grade_prompt, retriever_type)\n",
        "\n",
        "        tab1, tab2, tab3 = st.tabs([\"Results\", \"Question Scoring\", \"Ground Truth\"])\n",
        "\n",
        "        # Assemble outputs\n",
        "        metrics_df = pd.DataFrame(retrieval_metrics)\n",
        "        d = pd.DataFrame(predictions)\n",
        "\n",
        "        # Get the numerical score and the full reasoning\n",
        "        d['answer score'] = [int(re.search(r'\\d+', g['results']).group()) if re.search(r'\\d+', g['results']) else 0 for g in graded_answers]\n",
        "        d['reasoning'] = [re.search(r'Reasoning: (.+)', g['results'], flags=re.DOTALL).group(1).strip() if re.search(r'Reasoning: (.+)', g['results'], flags=re.DOTALL) else '' for g in graded_answers]\n",
        "\n",
        "        d['latency'] = latency\n",
        "\n",
        "\n",
        "        # Merge questions with respective metrics\n",
        "        metrics_df = metrics_df.drop(columns=['question'])\n",
        "        detailed_df = pd.concat([d, metrics_df], axis=1)\n",
        "        st.session_state.detailed_df = detailed_df\n",
        "\n",
        "        mean_latency = d['latency'].mean()\n",
        "        aggregate_metrics = metrics_df.mean()\n",
        "        answer_score = d['answer score'].mean()\n",
        "\n",
        "        # Show less metrics (values of '-') if st.session_state.gt_option == 'Generate Ground Truth from Documents'\n",
        "        new_row = {\n",
        "            'answer model': model,\n",
        "            'retriever': retriever_type,\n",
        "            'embedding': embeddings,\n",
        "            'chunk size': chunk_size,\n",
        "            'chunk overlap': chunk_overlap,\n",
        "            'k docs retrieved': num_neighbors,\n",
        "            'reranker': reranker_type,\n",
        "            'evaluation model': model_eval,\n",
        "            'evaluation questions': len(eval_set),\n",
        "            'Answer score': answer_score,\n",
        "            'Hit Rate': aggregate_metrics['Hit Rate'],\n",
        "            'MRR': aggregate_metrics['MRR'],\n",
        "            'NDCG': aggregate_metrics['NDCG'],\n",
        "            'Precision': aggregate_metrics.get('precision', '-'),\n",
        "            'Recall': aggregate_metrics.get('recall', '-'),\n",
        "            'AP': aggregate_metrics.get('AP', '-'),\n",
        "            'Latency': mean_latency,\n",
        "        }\n",
        "\n",
        "        # Append the new experiment data to the existing DataFrame\n",
        "        new_row_df = pd.DataFrame([new_row])\n",
        "        summary = pd.concat([summary, new_row_df], ignore_index=True)\n",
        "        st.session_state.existing_df = summary\n",
        "\n",
        "        # Define columns for 'Parameters' and 'Results'\n",
        "        parameters_columns = [\n",
        "            'answer model', 'retriever', 'embedding', 'chunk size', 'chunk overlap',\n",
        "            'k docs retrieved', 'reranker', 'evaluation model', 'evaluation questions'\n",
        "        ]\n",
        "        results_columns = [col for col in summary.columns if col not in parameters_columns]\n",
        "\n",
        "        # Tab 1: Aggregate Results of All Experiments\n",
        "        with tab1:\n",
        "            st.subheader(\"`Parameters of All Experiments`\")\n",
        "            parameters_df_all = summary[parameters_columns]\n",
        "            st.dataframe(parameters_df_all, use_container_width=True)\n",
        "\n",
        "            st.subheader(\"`Aggregate Results of All Experiments`\")\n",
        "            # Conditionally display results columns\n",
        "            if st.session_state.gt_option == 'Generate Ground Truth from Documents':\n",
        "                # Exclude 'Precision', 'Recall', 'AP' from the display\n",
        "                results_columns_filtered = [col for col in results_columns if col not in ['Precision', 'Recall', 'AP']]\n",
        "            else:\n",
        "                results_columns_filtered = results_columns\n",
        "\n",
        "            results_df_all = summary[results_columns_filtered]\n",
        "            st.dataframe(results_df_all, use_container_width=True)\n",
        "\n",
        "            # Dataframe for visualization\n",
        "            show = summary.reset_index()\n",
        "            show.columns = ['expt number', 'answer model', 'retriever', 'embedding', 'chunk size', 'chunk overlap', 'k docs retrieved', 'reranker', 'evaluation model', 'evaluation questions', 'Answer score', 'Precision', 'Recall', 'Hit Rate', 'MRR', 'AP', 'NDCG', 'Latency']\n",
        "            show['expt number'] = show['expt number'].apply(lambda x: \"Expt #: \" + str(x + 1))\n",
        "\n",
        "            # Determine the scale based on the value of grade_prompt\n",
        "            if grade_prompt == 'Binary':\n",
        "                scale_domain = [0, 1]\n",
        "                y_title = 'Binary Score'\n",
        "            elif grade_prompt == 'Score':\n",
        "                scale_domain = [0, 4]\n",
        "                y_title = 'Score'\n",
        "\n",
        "            # Set dimensions\n",
        "            chart_width = 400\n",
        "            chart_height = chart_width\n",
        "\n",
        "            c = alt.Chart(show, width=chart_width, height=chart_height).mark_circle().encode(\n",
        "                x=alt.X('NDCG', scale=alt.Scale(domain=[0, 1]), title='NDCG'),\n",
        "                y=alt.Y('Answer score', scale=alt.Scale(domain=scale_domain), title=y_title),\n",
        "                size='Latency',\n",
        "                color='expt number',\n",
        "                tooltip=['expt number', 'NDCG', 'Answer score', 'Latency'])\n",
        "\n",
        "            st.altair_chart(c, use_container_width=True)\n",
        "\n",
        "        # Tab 2: Detailed Results of the Last Experiment\n",
        "        with tab2:\n",
        "            st.subheader(\"`Parameters of Last Experiment`\")\n",
        "            # Parameters of the last experiment\n",
        "            parameters_df_last = summary.iloc[[-1]][parameters_columns]\n",
        "            st.dataframe(parameters_df_last, use_container_width=True)\n",
        "\n",
        "            st.subheader(\"`Question by Question Detail of Last Experiment`\")\n",
        "            # Modify metrics_df based on st.session_state.gt_option == 'Generate Ground Truth from Documents'\n",
        "            if st.session_state.gt_option == 'Generate Ground Truth from Documents':\n",
        "                detailed_df_filtered = st.session_state.detailed_df.drop(columns=['precision', 'recall', 'AP'], errors='ignore')\n",
        "            else:\n",
        "                detailed_df_filtered = st.session_state.detailed_df\n",
        "            st.dataframe(detailed_df_filtered, use_container_width=False)\n",
        "\n",
        "        # Display in Tab 3: Ground Truth Dataset\n",
        "        with tab3:\n",
        "            st.subheader(\"Ground Truth Data\")\n",
        "            st.dataframe(eval_set)\n",
        "\n",
        "            # Download button for ground truth data\n",
        "            json_content = json.dumps(eval_set)\n",
        "            st.download_button(\n",
        "                label=\"Download Ground Truth JSON\",\n",
        "                data=json_content,\n",
        "                file_name=\"ground_truth.json\",\n",
        "                mime=\"application/json\"\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-zWW5YjyEjw"
      },
      "source": [
        "# Run App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-t88xvhDwXyJ",
        "outputId": "2633b845-2623-4850-9a60-a9510b00c77a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.90.23.241\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py &>/content/logs.txt & curl ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH4PRVTBwblK",
        "outputId": "34194c5f-e8ca-4591-d58c-1533601ee684"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.591s\n",
            "your url is: https://free-yaks-fall.loca.lt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "DnSHD7QptwCv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "fQwFw7Bqy_Fw",
        "Mi-4fLsNXRys",
        "AuTCqL7AhJ-U"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}